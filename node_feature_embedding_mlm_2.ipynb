{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b48eab-c9fc-4697-b412-188235544562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8995d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 9999it [00:00, 30034.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing. Train sequences: 7962, Validation sequences: 1047, Test sequences: 991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Define nucleotide to integer mapping, including 'N' for unknown nucleotides\n",
    "nucleotide_to_index = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "unknown_nucleotide_index = 4  # Index for unknown nucleotides\n",
    "vocab_size = 5  # 4 nucleotides + 1 unknown ('N')\n",
    "\n",
    "input_file = '/mnt/f/hprc/hprc-v1.1-mc-chm13_segments.gfa'\n",
    "output_file = '/mnt/f/hprc/segments_b.hdf5'\n",
    "\n",
    "# Splitting probabilities\n",
    "train_prob = 0.8\n",
    "val_prob = 0.1\n",
    "test_prob = 0.1\n",
    "\n",
    "assert train_prob + val_prob + test_prob == 1.0, \"Split probabilities must sum to 1.0\"\n",
    "\n",
    "# Initialize counts\n",
    "train_count = 0\n",
    "val_count = 0\n",
    "test_count = 0\n",
    "\n",
    "# Open HDF5 file and create datasets\n",
    "with h5py.File(output_file, 'w') as hdf5_file:\n",
    "    dt = h5py.vlen_dtype(np.uint8)\n",
    "    train_dataset = hdf5_file.create_dataset('train_sequences', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "    val_dataset = hdf5_file.create_dataset('val_sequences', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "    test_dataset = hdf5_file.create_dataset('test_sequences', shape=(0,), maxshape=(None,), dtype=dt)\n",
    "    \n",
    "    # Open input file and read sequences\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in tqdm(f, desc=\"Processing sequences\"):\n",
    "            seq = line.strip().split(\"\\t\")[2]\n",
    "            if not seq:\n",
    "                continue  # Skip empty lines\n",
    "            \n",
    "            # Encode sequence to integers, mapping unknown nucleotides to unknown_nucleotide_index\n",
    "            encoded_seq = np.array(\n",
    "                [nucleotide_to_index.get(nuc.upper(), unknown_nucleotide_index) for nuc in seq],\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "            if len(encoded_seq) == 0:\n",
    "                continue  # Skip if sequence is empty after encoding\n",
    "            \n",
    "            # Randomly assign sequence to train, val, or test set\n",
    "            rand_num = random.random()\n",
    "            if rand_num < train_prob:\n",
    "                dataset = train_dataset\n",
    "                train_count += 1\n",
    "            elif rand_num < train_prob + val_prob:\n",
    "                dataset = val_dataset\n",
    "                val_count += 1\n",
    "            else:\n",
    "                dataset = test_dataset\n",
    "                test_count += 1\n",
    "            \n",
    "            # Resize dataset and store sequence\n",
    "            dataset.resize((dataset.shape[0] + 1,))\n",
    "            dataset[-1] = encoded_seq\n",
    "\n",
    "print(f\"Finished processing. Train sequences: {train_count}, Validation sequences: {val_count}, Test sequences: {test_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0bbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path, split='train', chunk_size=512, overlap=0, mask_prob=0.15):\n",
    "        self.hdf5_file_path = hdf5_file_path\n",
    "        self.split = split  # 'train', 'val', 'test'\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.mask_prob = mask_prob\n",
    "        self.vocab_size = 7  # 4 nucleotides + 1 unknown ('N') + 1 [MASK] token + 1 padding index\n",
    "\n",
    "        self.padding_index = self.vocab_size - 1  # Padding index\n",
    "        self.mask_token_index = self.vocab_size - 2  # [MASK] token index\n",
    "\n",
    "        self.hdf5_file = h5py.File(self.hdf5_file_path, 'r')\n",
    "        self.sequences = self.hdf5_file[f'{self.split}_sequences']\n",
    "        self.chunks = self._create_chunks()\n",
    "\n",
    "    def _create_chunks(self):\n",
    "        chunks = []\n",
    "        for idx, seq in enumerate(self.sequences):\n",
    "            seq_len = len(seq)\n",
    "            if seq_len == 0:\n",
    "                continue  # Skip empty sequences\n",
    "            if seq_len <= self.chunk_size:\n",
    "                chunks.append((idx, 0, seq_len))\n",
    "            else:\n",
    "                step = self.chunk_size - self.overlap\n",
    "                for start in range(0, seq_len - self.chunk_size + 1, step):\n",
    "                    end = start + self.chunk_size\n",
    "                    chunks.append((idx, start, end))\n",
    "                # Handle the last chunk if it doesn't fit exactly\n",
    "                if (seq_len - self.chunk_size) % step != 0:\n",
    "                    start = seq_len - self.chunk_size\n",
    "                    end = seq_len\n",
    "                    chunks.append((idx, start, end))\n",
    "        return chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, start, end = self.chunks[idx]\n",
    "        seq = self.sequences[seq_idx][start:end]\n",
    "        seq_tensor = torch.from_numpy(seq).long()  # Shape: (seq_len,)\n",
    "\n",
    "        # Apply masking for MLM\n",
    "        input_seq, labels = self._apply_masking(seq_tensor)\n",
    "\n",
    "        return input_seq, labels\n",
    "\n",
    "    def _apply_masking(self, seq_tensor):\n",
    "        seq_len = seq_tensor.size(0)\n",
    "        labels = seq_tensor.clone()  # Original labels\n",
    "\n",
    "        # Create mask for MLM\n",
    "        mask = torch.rand(seq_len) < self.mask_prob  # Decide which positions to mask\n",
    "        labels[~mask] = -100  # We only compute loss on masked positions\n",
    "\n",
    "        # Replace masked positions with [MASK] token index\n",
    "        input_seq = seq_tensor.clone()\n",
    "        input_seq[mask] = self.mask_token_index  # [MASK] token index\n",
    "\n",
    "        return input_seq, labels\n",
    "\n",
    "    def __del__(self):\n",
    "        self.hdf5_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435ad95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = [item[0] for item in batch]  # Each item[0] is a tensor of shape (seq_len,)\n",
    "    labels = [item[1] for item in batch]  # Each item[1] is a tensor of shape (seq_len,)\n",
    "\n",
    "    seq_lengths = [input_seq.size(0) for input_seq in inputs]\n",
    "    max_length = max(seq_lengths)\n",
    "    batch_size = len(inputs)\n",
    "\n",
    "    # Assuming padding_index is vocab_size - 1\n",
    "    padding_index = inputs[0].max().item() + 1  # Ensure padding index is unique\n",
    "    for seq in inputs:\n",
    "        padding_index = max(padding_index, seq.max().item() + 1)\n",
    "\n",
    "    # Initialize tensors for inputs and labels\n",
    "    padded_inputs = torch.full((batch_size, max_length), fill_value=padding_index, dtype=torch.long)\n",
    "    padded_labels = torch.full((batch_size, max_length), fill_value=-100, dtype=torch.long)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        seq_len = inputs[i].size(0)\n",
    "        padded_inputs[i, :seq_len] = inputs[i]\n",
    "        padded_labels[i, :seq_len] = labels[i]\n",
    "\n",
    "    return padded_inputs, padded_labels, padding_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e4576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, dim_feedforward, dropout=0.1, padding_idx=None):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=padding_idx)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size - 1)  # Exclude padding index\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_key_padding_mask=None):\n",
    "        # src: (batch_size, seq_len)\n",
    "        x = self.embedding(src)  # Shape: (batch_size, seq_len, d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = x.transpose(0, 1)  # Transformer expects input as (seq_len, batch_size, d_model)\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = x.transpose(0, 1)  # Back to (batch_size, seq_len, d_model)\n",
    "        output = self.output_layer(x)  # Shape: (batch_size, seq_len, vocab_size - 1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b4beb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamed/hprc_graph_embedding/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "100%|██████████| 250/250 [00:04<00:00, 53.68it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 196.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: nan, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:04<00:00, 55.03it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 182.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: nan, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 56/250 [00:00<00:03, 61.93it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     63\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 65\u001b[0m     total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     68\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DNADataset(hdf5_file_path=output_file, split='train', chunk_size=512, overlap=0)\n",
    "val_dataset = DNADataset(hdf5_file_path=output_file, split='val', chunk_size=512, overlap=0)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = train_dataset.vocab_size  # 4 nucleotides + 1 unknown + 1 [MASK] token + 1 padding index\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = TransformerModel(vocab_size=vocab_size, d_model=d_model, num_heads=num_heads,\n",
    "                         num_layers=num_layers, dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                         padding_idx=train_dataset.padding_index)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch_inputs, batch_labels, padding_index = batch\n",
    "        batch_inputs = batch_inputs.to(device)  # Shape: (batch_size, seq_len)\n",
    "        batch_labels = batch_labels.to(device)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Create src_key_padding_mask: True for padding positions\n",
    "        src_key_padding_mask = (batch_inputs == padding_index)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        outputs = model(batch_inputs, src_key_padding_mask=src_key_padding_mask)  # Shape: (batch_size, seq_len, vocab_size - 1)\n",
    "\n",
    "        # Shift labels to exclude padding index\n",
    "        adjusted_labels = batch_labels.clone()\n",
    "        adjusted_labels[adjusted_labels == padding_index] = -100  # Ignore padding in loss computation\n",
    "        adjusted_labels[adjusted_labels == model.embedding.padding_idx] = -100  # Ignore padding index in labels\n",
    "\n",
    "        # Ensure labels are in the range [0, vocab_size - 2]\n",
    "        adjusted_labels[adjusted_labels >= vocab_size - 1] = unknown_nucleotide_index\n",
    "\n",
    "        loss = criterion(outputs.view(-1, vocab_size - 1), adjusted_labels.view(-1))\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"Loss is NaN or Inf, skipping batch\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_inputs, batch_labels, padding_index = batch\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            src_key_padding_mask = (batch_inputs == padding_index)\n",
    "\n",
    "            outputs = model(batch_inputs, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "            adjusted_labels = batch_labels.clone()\n",
    "            adjusted_labels[adjusted_labels == padding_index] = -100\n",
    "            adjusted_labels[adjusted_labels == model.embedding.padding_idx] = -100\n",
    "            adjusted_labels[adjusted_labels >= vocab_size - 1] = unknown_nucleotide_index\n",
    "\n",
    "            loss = criterion(outputs.view(-1, vocab_size - 1), adjusted_labels.view(-1))\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
