{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b48eab-c9fc-4697-b412-188235544562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f4d096-7076-4484-bc03-cc739756094a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fce25fd1330>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f723f69-af86-4057-9812-e6a7e557516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and HDF5 Creation\n",
    "# Define nucleotide mapping\n",
    "nucleotides = ['A', 'C', 'G', 'T']\n",
    "token2idx = {nuc: idx + 2 for idx, nuc in enumerate(nucleotides)}  # 'A':2, 'C':3, 'G':4, 'T':5\n",
    "token2idx['PAD'] = 0  # Padding token\n",
    "token2idx['MASK'] = 1  # Mask token\n",
    "idx2token = {idx: nuc for nuc, idx in token2idx.items()}\n",
    "vocab_size = len(token2idx)  # Should be 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68b19545-1503-41ec-b58c-bb35d0fccfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode sequences\n",
    "def encode_sequence_to_array(seq):\n",
    "    seq = seq.upper()\n",
    "    encoded_seq = [token2idx.get(nuc, token2idx['PAD']) for nuc in seq]  # Map unknown nucleotides to 'PAD'\n",
    "    return np.array(encoded_seq, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d064ff9-d46d-4aab-a8b4-cd1e93ac7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode sequences\n",
    "def encode_sequence(seq, max_length):\n",
    "    seq = seq.upper()\n",
    "    seq_encoded = [token2idx.get(nuc, token2idx['PAD']) for nuc in seq]\n",
    "    # Pad sequences\n",
    "    if len(seq_encoded) < max_length:\n",
    "        seq_encoded += [token2idx['PAD']] * (max_length - len(seq_encoded))\n",
    "    else:\n",
    "        seq_encoded = seq_encoded[:max_length]\n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c075fe52-0903-43e3-836a-22789cdb5455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to input and output files\n",
    "input_file = '/mnt/f/hprc/segments_b.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d735cd0-6d25-4a85-bf73-8bfcb10c2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split probabilities\n",
    "train_prob = 0.8\n",
    "val_prob = 0.1\n",
    "test_prob = 0.1\n",
    "assert train_prob + val_prob + test_prob == 1.0, \"Split probabilities must sum to 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf42b1ef-1cd3-441a-9f0e-f2fa779a1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset and DataLoader\n",
    "# Function to mask sequences for MLM\n",
    "def mask_sequence(seq_encoded, mask_prob=0.15):\n",
    "    input_ids = seq_encoded.copy()\n",
    "    labels = [-100] * len(seq_encoded)  # Initialize labels with -100 (ignore index)\n",
    "    for i in range(len(seq_encoded)):\n",
    "        if seq_encoded[i] == token2idx['PAD']:\n",
    "            continue  # Skip padding tokens\n",
    "        if random.random() < mask_prob:\n",
    "            labels[i] = seq_encoded[i]  # Save the original token id for loss calculation\n",
    "            input_ids[i] = token2idx['MASK']  # Replace with [MASK] token\n",
    "    return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0e02e31-1e0b-49e8-add6-a6171f2b0158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path, split='train', mask_prob=0.15, max_seq_len=512):\n",
    "        self.hdf5_file_path = hdf5_file_path\n",
    "        self.split = split\n",
    "        self.mask_prob = mask_prob\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.hdf5_file = None  # Will be opened lazily in __getitem__\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.hdf5_file is None:\n",
    "            with h5py.File(self.hdf5_file_path, 'r') as hdf5_file:\n",
    "                self.length = len(hdf5_file[f'{self.split}_sequences'])\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.hdf5_file is None:\n",
    "            # Each worker opens its own file handle\n",
    "            self.hdf5_file = h5py.File(self.hdf5_file_path, 'r')\n",
    "            self.dataset = self.hdf5_file[f'{self.split}_sequences']\n",
    "        \n",
    "        seq_encoded = self.dataset[idx].tolist()\n",
    "        \n",
    "        # Truncate the sequence to max_seq_len\n",
    "        if len(seq_encoded) > self.max_seq_len:\n",
    "            seq_encoded = seq_encoded[:self.max_seq_len]\n",
    "        \n",
    "        input_ids, labels = mask_sequence(seq_encoded, mask_prob=self.mask_prob)\n",
    "        return input_ids, labels\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.hdf5_file is not None:\n",
    "            self.hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path, chunk_size=512, overlap=0, vocab_size=4):\n",
    "        self.hdf5_file_path = hdf5_file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hdf5_file = h5py.File(self.hdf5_file_path, 'r')\n",
    "        self.sequences = self.hdf5_file['sequences']\n",
    "        self.chunks = self._create_chunks()\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.hdf5_file is None:\n",
    "            with h5py.File(self.hdf5_file_path, 'r') as hdf5_file:\n",
    "                self.length = len(hdf5_file[f'{self.split}_sequences'])\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq_idx, start, end = self.chunks[idx]\n",
    "        seq = self.sequences[seq_idx][start:end]\n",
    "        seq_tensor = torch.from_numpy(seq).long()  # Shape: (seq_len,)\n",
    "\n",
    "        # Convert to one-hot encoding\n",
    "        one_hot_seq = F.one_hot(seq_tensor, num_classes=self.vocab_size).float()  # Shape: (seq_len, vocab_size)\n",
    "        return one_hot_seq\n",
    "\n",
    "    def __del__(self):\n",
    "        self.hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6dce62b-d523-4364-938b-8e13d587fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c273e052-a023-4b83-8471-1184f2e4faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function for variable-length sequences\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(item[0], dtype=torch.long) for item in batch]\n",
    "    labels = [torch.tensor(item[1], dtype=torch.long) for item in batch]\n",
    "    \n",
    "    # Truncate sequences to max_seq_len\n",
    "    input_ids = [seq[:max_seq_len] for seq in input_ids]\n",
    "    labels = [lbl[:max_seq_len] for lbl in labels]\n",
    "    \n",
    "    # Pad sequences to the maximum length in the batch (which will be <= max_seq_len)\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=token2idx['PAD']\n",
    "    )\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100  # -100 is the ignore index for loss\n",
    "    )\n",
    "    return input_ids_padded, labels_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "219f72cb-2b2e-440f-aae2-3a435f4e5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each split\n",
    "train_dataset = HDF5Dataset(hdf5_file_path=output_file, split='train', mask_prob=0.15)\n",
    "val_dataset = HDF5Dataset(hdf5_file_path=output_file, split='val', mask_prob=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5557ea0e-d3a1-4aa9-b236-0f3a4d8683eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "#train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,  # Further reduce if necessary\n",
    "    shuffle=True,\n",
    "    num_workers=2,  # Reduce from 4 to 2 or 1\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "971466a1-a9b5-442b-82b7-e014c4870cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "# Positional Encoding class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        device = x.device\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)  # (seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float, device=device) * \n",
    "                             (-np.log(10000.0) / self.d_model))\n",
    "\n",
    "        pe = torch.zeros(seq_len, self.d_model, device=device)  # (seq_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "        x = x + pe\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0df9c0d-a511-4ecb-9199-16d63934a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Model\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, dff, num_layers, dropout_rate=0.1):\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=token2idx['PAD'])\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, \n",
    "                                                   dim_feedforward=dff, dropout=dropout_rate)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (batch_size, seq_len)\n",
    "        src_key_padding_mask = src == token2idx['PAD']  # (batch_size, seq_len)\n",
    "        x = self.embedding(src) * np.sqrt(self.embedding.embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = x.transpose(0, 1)  # (seq_len, batch_size, d_model)\n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = x.transpose(0, 1)  # (batch_size, seq_len, d_model)\n",
    "        logits = self.fc_out(x)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1621d530-2303-4880-bfcd-1a2a299d2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "975a024f-8e99-48ba-9671-0c1717977990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "dff = 256\n",
    "num_layers = 2\n",
    "dropout_rate = 0.1\n",
    "max_seq_len = 520  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3dbba25-3ba1-4289-899e-76ed8bf3693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = TransformerEncoderModel(\n",
    "    vocab_size=vocab_size, \n",
    "    d_model=d_model, \n",
    "    num_heads=num_heads,\n",
    "    dff=dff, \n",
    "    num_layers=num_layers, \n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d350e08e-7788-448d-ae2c-e7fd1454dff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderModel(\n",
       "  (embedding): Embedding(6, 64, padding_idx=0)\n",
       "  (pos_encoder): PositionalEncoding()\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e40245e-47c5-4459-b5a0-7a97faa79e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69dd3427-643d-4e70-9359-7536b18bf8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4b7ce89-94f3-4973-8f4f-d32ee18beacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:19<00:00, 129.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 1.3578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Loss: 1.3443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:16<00:00, 149.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training Loss: 1.3485\n",
      "Epoch 2/10, Validation Loss: 1.3440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:18<00:00, 137.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training Loss: 1.3477\n",
      "Epoch 3/10, Validation Loss: 1.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:17<00:00, 140.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training Loss: 1.3464\n",
      "Epoch 4/10, Validation Loss: 1.3328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:20<00:00, 123.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training Loss: 1.3455\n",
      "Epoch 5/10, Validation Loss: 1.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:20<00:00, 123.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training Loss: 1.3431\n",
      "Epoch 6/10, Validation Loss: 1.3431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:16<00:00, 150.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training Loss: 1.3431\n",
      "Epoch 7/10, Validation Loss: 1.3354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:20<00:00, 124.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Training Loss: 1.3455\n",
      "Epoch 8/10, Validation Loss: 1.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:18<00:00, 138.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Training Loss: 1.3443\n",
      "Epoch 9/10, Validation Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2506/2506 [00:20<00:00, 124.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Training Loss: 1.3416\n",
      "Epoch 10/10, Validation Loss: 1.3395\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    #for batch_input_ids, batch_labels in train_loader:\n",
    "    for batch_input_ids, batch_labels in tqdm(train_loader):\n",
    "        batch_input_ids = batch_input_ids.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_input_ids)  # (batch_size, seq_len, vocab_size)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), batch_labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input_ids, batch_labels in val_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_input_ids)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), batch_labels.view(-1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68ef399b-414a-4ecd-b8cb-2860390ae0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     # Training Phase\n",
    "#     model.train()\n",
    "#     total_train_loss = 0\n",
    "#     #for batch_input_ids, batch_labels in train_loader:\n",
    "#     for batch_input_ids, batch_labels in tqdm(train_loader):\n",
    "#         batch_input_ids = batch_input_ids.to(device)\n",
    "#         batch_labels = batch_labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_input_ids)  # (batch_size, seq_len, vocab_size)\n",
    "#         loss = criterion(outputs.view(-1, vocab_size), batch_labels.view(-1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_train_loss += loss.item()\n",
    "\n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "#     print(f'Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "#     # Validation Phase\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch_input_ids, batch_labels in val_loader:\n",
    "#             batch_input_ids = batch_input_ids.to(device)\n",
    "#             batch_labels = batch_labels.to(device)\n",
    "\n",
    "#             outputs = model(batch_input_ids)\n",
    "#             loss = criterion(outputs.view(-1, vocab_size), batch_labels.view(-1))\n",
    "#             total_val_loss += loss.item()\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "#     print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5965f8f6-aef9-48d0-bacb-c5917821789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_feature_vector(sequence):\n",
    "    model.eval()\n",
    "\n",
    "    # Encode sequence and convert to tensor\n",
    "    seq_encoded = encode_sequence(sequence, max_seq_len)\n",
    "    seq_tensor = torch.tensor([seq_encoded], dtype=torch.long).to(device)  # Move to specified device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        batch_size, seq_len = seq_tensor.size()\n",
    "\n",
    "        # Apply embeddings and positional encoding\n",
    "        x = model.embedding(seq_tensor) * np.sqrt(model.embedding.embedding_dim)  # Embedding scaling\n",
    "        x = model.pos_encoder(x)  # Apply positional encoding\n",
    "\n",
    "        # Apply transformer encoder layers\n",
    "        x = x.transpose(0, 1)  # (seq_len, batch_size, d_model)\n",
    "        x = model.encoder(x)\n",
    "        x = x.transpose(0, 1)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Pooling to get feature vector\n",
    "        feature_vector = x.mean(dim=1).squeeze(0).cpu().numpy()  # Move to CPU before converting to numpy\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c9907af-bfd0-4bec-bd09-c2cf8032e00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Vector (shape (64,)):\n",
      "[ 5.8099587e-02 -6.1848026e-02  6.0076017e-02 -4.3027827e-01\n",
      " -2.8560886e-01  3.1843016e-01  4.0122762e-02  5.3647733e-01\n",
      " -1.0901927e+00  1.6983964e-02  2.1897623e-02 -3.3709139e-02\n",
      " -5.7947643e-02 -4.0864423e-03  1.1620720e-02  3.5534676e-02\n",
      "  4.3646872e-02  2.5381869e-02  3.2397246e-01 -1.0952059e+00\n",
      "  2.0940056e-01 -4.1635954e-03 -1.0789967e-02  8.6933456e-02\n",
      " -2.3620518e-01 -4.8311031e-04  2.1125045e-02  4.4094608e-03\n",
      "  3.2754294e-03 -2.8062798e-02 -1.9266700e-02  4.1266456e-02\n",
      " -6.1162031e-01 -5.6737955e-03 -9.3305456e-03  1.2559577e-02\n",
      " -1.8635046e-02  8.6001754e-03  9.2126913e-02 -1.1084046e-01\n",
      " -4.7628932e-02  2.2788800e-01  8.9139994e-03 -1.2620354e-01\n",
      "  1.1468509e-01 -1.6168781e-01 -1.0524358e-02 -1.6406955e-01\n",
      " -3.4234583e-02 -3.6349267e-02  2.7449679e-02  7.0067993e-03\n",
      "  1.5577413e-01 -1.5072285e-02 -1.5130562e-02 -2.5287632e-02\n",
      " -5.3113955e-03  5.4835291e-03 -2.5302169e-03  2.5189964e-02\n",
      " -4.1460890e-02 -9.7081564e-02  7.1518809e-02 -1.1278626e-02]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sequence = 'ACGTGCTAGC'\n",
    "feature_vector = sequence_to_feature_vector(sequence)\n",
    "print(f\"Feature Vector (shape {feature_vector.shape}):\\n{feature_vector}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
